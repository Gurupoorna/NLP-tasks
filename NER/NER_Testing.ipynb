{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLDANarisLUL",
        "outputId": "06e2f713-943e-4d11-efca-b679aea27879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1153 correct out of 1185 entities. Accuracy = 0.9729957805907173\n",
            "[0 0 0 1 1 0 1 0 0 0] [0 0 0 1 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "import nltk\n",
        "from nltk.corpus.reader import ConllCorpusReader\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "import re\n",
        "from copy import copy, deepcopy\n",
        "\n",
        "train = ConllCorpusReader('CoNLL-2003', 'eng.train', ['words', 'pos', 'ignore', 'chunk'])\n",
        "test = ConllCorpusReader('CoNLL-2003', 'eng.testa', ['words', 'pos', 'ignore', 'chunk'])\n",
        "\n",
        "def load_gazetteer_dict():\n",
        "    with open('gazetteer.txt') as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [i.strip() for i in lines]\n",
        "        g_dict = defaultdict(set)\n",
        "        for line in lines:\n",
        "            tag, word = line.split()[0], (' ').join(line.split()[1:])\n",
        "            g_dict[tag].add(word)\n",
        "    return g_dict\n",
        "\n",
        "g_dict = load_gazetteer_dict()\n",
        "\n",
        "# Function to generate word-level features\n",
        "def word2features(i, wordseq):\n",
        "    wi = wordseq[i]\n",
        "    features = defaultdict(lambda: -1)\n",
        "\n",
        "    if wi == '<START>' or wi == '<STOP>':\n",
        "        features.update({\n",
        "            wi: True\n",
        "        })\n",
        "        return features\n",
        "\n",
        "    features.update({\n",
        "        'Wiaslower': wi.lower(),\n",
        "        'iswialpha': wi.isalpha(),\n",
        "        'iswititle': wi.istitle(),\n",
        "        'iswiupper': wi.isupper(),\n",
        "        'iswilower': wi.islower(),\n",
        "        'iswidigit': wi.isdigit(),\n",
        "        'iswinumeric': wi.isnumeric(),\n",
        "        'Wishape': len(wi),\n",
        "    })\n",
        "\n",
        "    if i > 1:\n",
        "        wiminus1 = wordseq[i - 1]\n",
        "        features.update({\n",
        "            'iswiminus1title': wiminus1.istitle(),\n",
        "            'iswiminus1upper': wiminus1.isupper(),\n",
        "            'iswiminus1lower': wiminus1.islower(),\n",
        "            'Wi-1aslower': wiminus1.lower(),\n",
        "        })\n",
        "    elif i == 1:\n",
        "        features.update({\n",
        "            'BOS': True,\n",
        "        })\n",
        "\n",
        "    if i < len(wordseq) - 2:\n",
        "        wiplus1 = wordseq[i + 1]\n",
        "        features.update({\n",
        "            'iswiplus1title': wiplus1.istitle(),\n",
        "            'iswiplus1upper': wiplus1.isupper(),\n",
        "            'iswiplus1lower': wiplus1.islower(),\n",
        "            'Wi+1aslower': wiplus1.lower(),\n",
        "        })\n",
        "    elif i == len(wordseq) - 2:\n",
        "        features.update({\n",
        "            'EOS': True,\n",
        "        })\n",
        "\n",
        "    if wi != '.' or not wi.isnumeric():\n",
        "        gaz = False\n",
        "        for k in g_dict.keys():\n",
        "            if wi in g_dict[k]:\n",
        "                gaz = True\n",
        "                features.update({\n",
        "                    'gaztag-' + str(k): 1,\n",
        "                })\n",
        "        features.update({'gaz': gaz})\n",
        "    else:\n",
        "        features.update({'gaz': -1})\n",
        "\n",
        "    return features\n",
        "\n",
        "# Function to convert sentence into features\n",
        "def sent2features(sentence):\n",
        "    assert isinstance(sentence, list) and isinstance(sentence[0], str), '`sentence` should be list of words as str'\n",
        "    assert sentence[0]=='<START>' and sentence[-1]=='<STOP>' , '`sentence` should have <START> and <STOP> tags'\n",
        "    xs = [None] * len(sentence)\n",
        "    for i in range(len(sentence)):\n",
        "        xs[i] = word2features(i, sentence)\n",
        "    return xs\n",
        "\n",
        "# Label Encoder to return 1 for B- and I- tags, 0 otherwise\n",
        "def ylabel_encode_decode(y, todo='encode'):\n",
        "    if todo == 'encode':\n",
        "        ty = [1 if label.startswith('B-') or label.startswith('I-') else 0 for label in y]\n",
        "    return ty\n",
        "\n",
        "# Function to vectorize the features and labels from sentences in IOB format\n",
        "def iob_sents2Xy(iob_sents, test=False):\n",
        "    Xs = list(chain.from_iterable([['<START>'] + [w for w, _, _ in wseq] + ['<STOP>'] for wseq in iob_sents]))\n",
        "    y = list(chain.from_iterable([['<START>'] + [e for _, _, e in wseq] + ['<STOP>'] for wseq in iob_sents]))\n",
        "\n",
        "    y = ylabel_encode_decode(y)\n",
        "\n",
        "    X = sent2features(Xs)\n",
        "    X = feats2vects(X, test=test)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "d2v = DictVectorizer(sparse=False)\n",
        "\n",
        "# Function to transform features into vectors\n",
        "def feats2vects(features, test=False):\n",
        "    if not test:\n",
        "        return d2v.fit_transform(features)\n",
        "    else:\n",
        "        return d2v.transform(features)\n",
        "\n",
        "m = 1000\n",
        "\n",
        "# Prepare training data\n",
        "Xtrain, ytrain = iob_sents2Xy(train.iob_sents()[:m])\n",
        "\n",
        "svmclassifier = SVC()\n",
        "svmclassifier.fit(Xtrain, ytrain)\n",
        "\n",
        "Xtest, ytest = iob_sents2Xy(train.iob_sents()[m:m + 100], test=True)\n",
        "\n",
        "predictions = svmclassifier.predict(Xtest)\n",
        "\n",
        "print(f'{(predictions == ytest).sum()} correct out of {ytest.shape[0]} entities. '\n",
        "      f'Accuracy = {(predictions == ytest).sum() / ytest.shape[0]}')\n",
        "\n",
        "print(predictions[:10], ytest[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nctiYEK-ulju"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1] \n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1] \n",
            " [['SOCCER', '-', 'UEFA', 'REWARDS', 'THREE', 'COUNTRIES', 'FOR', 'FAIR', 'PLAY', '.'], ['GENEVA', '1996-08-22'], ['Norway', ',', 'England', 'and', 'Sweden', 'were', 'rewarded', 'for', 'their', 'fair', 'play', 'on', 'Thursday', 'with', 'an', 'additional', 'place', 'in', 'the', '1997-98', 'UEFA', 'Cup', 'competition', '.'], ['Norway', 'headed', 'the', 'UEFA', 'Fair', 'Play', 'rankings', 'for', '1995-96', 'with', '8.62', 'points', ',', 'ahead', 'of', 'England', 'with', '8.61', 'and', 'Sweden', '8.57', '.'], ['The', 'rankings', 'are', 'based', 'on', 'a', 'formula', 'that', 'takes', 'into', 'account', 'many', 'factors', 'including', 'red', 'and', 'yellow', 'cards', ',', 'and', 'coaching', 'and', 'spectators', \"'\", 'behaviour', 'at', 'matches', 'played', 'at', 'an', 'international', 'level', 'by', 'clubs', 'and', 'national', 'teams', '.'], ['Only', 'the', 'top', 'three', 'countries', 'are', 'allocated', 'additional', 'places', '.'], ['The', 'UEFA', 'Fair', 'Play', 'rankings', 'are', ':', '1', '.'], ['Norway', '8.62', 'points'], ['2.', 'England', '8.61'], ['3.', 'Sweden', '8.57'], ['4.', 'Faroe', 'Islands', '8.56'], ['5.', 'Wales', '8.54'], ['6.', 'Estonia', '8.52'], ['7.', 'Ireland', '8.45'], ['8.', 'Belarus', '8.39'], ['9.', 'Iceland', '8.35'], ['10.', 'Netherlands', '8.30'], ['10.', 'Denmark', '8.30'], ['10.', 'Germany', '8.30'], ['13.', 'Scotland', '8.29']]\n"
          ]
        }
      ],
      "source": [
        "k = 20\n",
        "print(predictions[:k],'\\n', ytest[:k],'\\n',[[w for w, _, _ in s] for s in train.iob_sents()[m:m+k]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
