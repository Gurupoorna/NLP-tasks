{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "train = ConllCorpusReader('CoNLL-2003', 'eng.train', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test = ConllCorpusReader('CoNLL-2003', 'eng.testa', ['words', 'pos', 'ignore', 'chunk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LOC', 'MISC', 'ORG', 'PER'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_gazetteer_dict():\n",
    "    with open('./gazetteer.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [i[:-1] for i in lines]\n",
    "        g_dict = defaultdict(set)\n",
    "        for line in lines:\n",
    "            tag, word = line.split()[0], (' ').join(line.split()[1:])\n",
    "            g_dict[tag].add(word) #stores a set of words for each tag\n",
    "    \n",
    "    # print ('gazetteer dict sample: ',g_dict.keys())\n",
    "    return g_dict\n",
    "g_dict = load_gazetteer_dict()\n",
    "g_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "def word2features(i, wordseq):\n",
    "    wi = wordseq[i]\n",
    "    features = defaultdict(lambda x: -1)\n",
    "    if wi == '<START>' or wi == '<STOP>':\n",
    "        features.update({\n",
    "            wi: True\n",
    "        })\n",
    "        return features\n",
    "    \n",
    "    features.update({\n",
    "        # 'Wi': wi,\n",
    "        'Wiaslower': wi.lower(),\n",
    "        'iswialpha': wi.isalpha(),\n",
    "        'iswititle': wi.istitle(),\n",
    "        'iswiupper': wi.isupper(),\n",
    "        'iswilower': wi.islower(),\n",
    "        'iswidigit': wi.isdigit(),\n",
    "        'iswinumeric': wi.isnumeric(),\n",
    "        'Wishape': len(wi),\n",
    "        # 'ishyphens': re.match('-',wi) == None,\n",
    "\n",
    "    })\n",
    "    if i>1:\n",
    "        wiminus1 = wordseq[i-1]\n",
    "        features.update({\n",
    "            # 'Wi-1': wiminus1,\n",
    "            'iswiminus1title': wiminus1.istitle(),\n",
    "            'iswiminus1upper': wiminus1.isupper(),\n",
    "            'iswiminus1lower': wiminus1.islower(),\n",
    "            'Wi-1aslower': wiminus1.lower(),\n",
    "        })\n",
    "    elif i==1:\n",
    "        features.update({\n",
    "            'BOS':True,\n",
    "        })\n",
    "    if i<len(wordseq)-2:\n",
    "        wiplus1 = wordseq[i+1]\n",
    "        features.update({\n",
    "            # 'Wi+1': wiplus1,\n",
    "            'iswiplus1title': wiplus1.istitle(),\n",
    "            'iswiplus1upper': wiplus1.isupper(),\n",
    "            'iswiplus1lower': wiplus1.islower(),\n",
    "            'Wi+1aslower': wiplus1.lower(),\n",
    "        })\n",
    "    elif i==len(wordseq)-2:\n",
    "        features.update({\n",
    "            'EOS':True,\n",
    "        })\n",
    "    if wi != '.':\n",
    "        gaz = False\n",
    "        for k in g_dict.keys():\n",
    "            if wi in g_dict[k]:\n",
    "                gaz = True\n",
    "                features.update({\n",
    "                    'gaztag-'+str(k): 1,\n",
    "                })\n",
    "        features.update({'gaz': gaz})\n",
    "    else : features.update({'gaz': -1})\n",
    "    return features\n",
    "\n",
    "def sent2features(sentence):\n",
    "    assert isinstance(sentence, list) and isinstance(sentence[0], str), '`sentence` should be list of words as str'\n",
    "    xs = [None]*len(sentence)\n",
    "    for i in range(len(sentence)):\n",
    "        xs[i] = word2features(i, sentence)\n",
    "    return xs\n",
    "\n",
    "dummyfeature = [word2features(i,['<START>','aaa','China','Matthew','Jordans','AaA9','123','<STOP>']) for i in range(8)]\n",
    "d2v = DictVectorizer(sparse=False)\n",
    "d2v.fit(dummyfeature)\n",
    "save_transform = None\n",
    "red2v = DictVectorizer(sparse=False)\n",
    "def feats2vects(features, test=False, refit=True):\n",
    "    if not refit and not test: tX = d2v.transform(features)\n",
    "    elif refit and not test:\n",
    "        tX = red2v.fit_transform(features)\n",
    "    # print(tX.shape)\n",
    "    elif test:\n",
    "        tX = red2v.transform(features)\n",
    "    return tX\n",
    "\n",
    "# def sent2featvects(sentence):\n",
    "#     xs = sent2features(sentence)\n",
    "#     tx = feats2vects(xs)\n",
    "#     # print(tx.shape)\n",
    "#     return tx\n",
    "\n",
    "\n",
    "\n",
    "all_labels = ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O', '<START>', '<STOP>']                 \n",
    "le = LabelEncoder()\n",
    "le.fit(all_labels)\n",
    "def ylabel_encode_decode(y, todo='encode'):\n",
    "    if todo == 'encode':\n",
    "        ty = le.transform(y)\n",
    "    elif todo == 'decode':\n",
    "        ty = le.inverse_transform(y)\n",
    "    # print(ty.shape)\n",
    "    return ty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "def iob_sents2Xy(iob_sents, test=False) :\n",
    "    Xs = list(chain.from_iterable([['<START>']+[w for w, _, _ in wseq]+['<STOP>'] for wseq in iob_sents]))\n",
    "    y = list(chain.from_iterable([['<START>']+[e for _, _, e in wseq]+['<STOP>'] for wseq in iob_sents]))\n",
    "    X = sent2features(Xs)\n",
    "    X = feats2vects(X, test=test)\n",
    "        # for wseq in iob_sents:\n",
    "        #     xi = []\n",
    "        #     y.append('<START>')\n",
    "        #     xi.extend(sent2featvects(['<START>']+[w for w, _, _ in wseq]+['<STOP>']))\n",
    "        #     y.extend([entity for w, _, entity in wseq])\n",
    "        #     y.append('<STOP>')\n",
    "        #     X.extend(xi)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13653, 9531), (13653,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "Xtrain, ytrain = iob_sents2Xy(train.iob_sents()[:m])\n",
    "Xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1073 correct out of 1185 entities. Accuracy = 0.9054852320675105\n",
      "['<START>' 'O' 'O' 'B-ORG' 'O' 'O' 'O' 'O' 'O' 'O'] ['<START>' 'O' 'O' 'B-ORG' 'O' 'O' 'O' 'O' 'O' 'O']\n"
     ]
    }
   ],
   "source": [
    "svmclassifier = SVC()\n",
    "\n",
    "svmclassifier.fit(Xtrain,ytrain)\n",
    "\n",
    "Xtest, ytest = iob_sents2Xy(train.iob_sents()[m:m+100], test=True)\n",
    "\n",
    "predictions = svmclassifier.predict(Xtest)\n",
    "\n",
    "print(f'{(predictions == ytest).sum()} correct out of {ytest.shape[0]} entities. '\n",
    "      f'Accuracy = {(predictions == ytest).sum()/ytest.shape[0]}')\n",
    "print(predictions[:10], ytest[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'<START>': 1.0},\n",
       " {'Wi+1aslower=-': 1.0,\n",
       "  'Wiaslower=soccer': 1.0,\n",
       "  'Wishape': 6.0,\n",
       "  'iswialpha': 1.0,\n",
       "  'iswiupper': 1.0},\n",
       " {'Wi-1aslower=soccer': 1.0,\n",
       "  'Wiaslower=-': 1.0,\n",
       "  'Wishape': 1.0,\n",
       "  'iswiminus1upper': 1.0,\n",
       "  'iswiplus1upper': 1.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red2v.inverse_transform(Xtest[:3])\n",
    "# d2v.inverse_transform(d2v.transform(dummyfeature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOCCER', 'NN', 'O'),\n",
       " ('-', ':', 'O'),\n",
       " ('UEFA', 'NNP', 'B-ORG'),\n",
       " ('REWARDS', 'NNPS', 'O'),\n",
       " ('THREE', 'CD', 'O'),\n",
       " ('COUNTRIES', 'NNS', 'O'),\n",
       " ('FOR', 'IN', 'O'),\n",
       " ('FAIR', 'NNP', 'O'),\n",
       " ('PLAY', 'NNP', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iob_sents()[m]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaarya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
