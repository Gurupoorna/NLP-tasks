{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "train = ConllCorpusReader('CoNLL-2003', 'eng.train', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test = ConllCorpusReader('CoNLL-2003', 'eng.testa', ['words', 'pos', 'ignore', 'chunk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LOC', 'MISC', 'ORG', 'PER'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_gazetteer_dict():\n",
    "    with open('./gazetteer.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [i[:-1] for i in lines]\n",
    "        g_dict = defaultdict(set)\n",
    "        for line in lines:\n",
    "            tag, word = line.split()[0], (' ').join(line.split()[1:])\n",
    "            g_dict[tag].add(word) #stores a set of words for each tag\n",
    "    \n",
    "    # print ('gazetteer dict sample: ',g_dict.keys())\n",
    "    return g_dict\n",
    "g_dict = load_gazetteer_dict()\n",
    "g_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "def word2features(i, wordseq):\n",
    "    wi = wordseq[i]\n",
    "    features = defaultdict(lambda : -1)\n",
    "    assert wordseq[0] == '<START>' and wordseq[-1] == '<STOP>', \"<start> and <stop> tags missing\"\n",
    "    if wi == '<START>' or wi == '<STOP>':\n",
    "        features.update({\n",
    "            wi: True\n",
    "        })\n",
    "        return features\n",
    "    \n",
    "    alphabet = list('abcdefghijklmnopqrstuvwxyz')\n",
    "    bow = dict()\n",
    "    for char in alphabet:\n",
    "        bow['$'+char] = 0\n",
    "    for char in wi.lower():\n",
    "        if char in alphabet:\n",
    "            bow['$'+char] += 1\n",
    "\n",
    "    features.update(bow)\n",
    "\n",
    "    features.update({\n",
    "        # 'Wi': wi,\n",
    "        # 'Wiaslower': wi.lower(),\n",
    "        'iswialpha': wi.isalpha(),\n",
    "        'iswititle': wi.istitle(),\n",
    "        'iswiupper': wi.isupper(),\n",
    "        'iswilower': wi.islower(),\n",
    "        'iswidigit': wi.isdigit(),\n",
    "        'iswinumeric': wi.isnumeric(),\n",
    "        'Wishape': len(wi),\n",
    "        'hasspecialchar': len([w for w in wi if w in list('~!@#$%^&*()_+-={}|:\"<>?,./;\\'[]')]) != 0   #re.findall('[~!@#$%^&*(){}|:\"<>?,./;\\'\\[\\]]', wi) != None,\n",
    "    })\n",
    "    if i>1:\n",
    "        wiminus1 = wordseq[i-1]\n",
    "        features.update({\n",
    "            # 'Wi-1': wiminus1,\n",
    "            'iswiminus1title': wiminus1.istitle(),\n",
    "            'iswiminus1upper': wiminus1.isupper(),\n",
    "            'iswiminus1lower': wiminus1.islower(),\n",
    "            # 'Wi-1aslower': wiminus1.lower(),\n",
    "        })\n",
    "    elif i==1:\n",
    "        features.update({\n",
    "            'BOS':True,\n",
    "            'iswiminus1title': -1,\n",
    "            'iswiminus1upper': -1,\n",
    "            'iswiminus1lower': -1,\n",
    "        })\n",
    "    if i<len(wordseq)-2:\n",
    "        wiplus1 = wordseq[i+1]\n",
    "        features.update({\n",
    "            # 'Wi+1': wiplus1,\n",
    "            'iswiplus1title': wiplus1.istitle(),\n",
    "            'iswiplus1upper': wiplus1.isupper(),\n",
    "            'iswiplus1lower': wiplus1.islower(),\n",
    "            # 'Wi+1aslower': wiplus1.lower(),\n",
    "        })\n",
    "    elif i==len(wordseq)-2:\n",
    "        features.update({\n",
    "            'EOS':True,\n",
    "            'iswiplus1title': -1,\n",
    "            'iswiplus1upper': -1,\n",
    "            'iswiplus1lower': -1,\n",
    "        })\n",
    "\n",
    "    features.update({\n",
    "        \"gaz\" : -1, \"gaz+1\" : -1, \"gaz-1\" : -1, \"gaz3\" : -1,\n",
    "        'gaztag-LOC' : -1, 'gaztag-PER' : -1, 'gaztag-ORG' : -1, 'gaztag-MISC' : -1,\n",
    "        'gaz+1tag-LOC' : -1, 'gaz+1tag-PER' : -1, 'gaz+1tag-ORG' : -1, 'gaz+1tag-MISC' : -1,\n",
    "        'gaz-1tag-LOC' : -1, 'gaz-1tag-PER' : -1, 'gaz-1tag-ORG' : -1, 'gaz-1tag-MISC' : -1,  \n",
    "    })\n",
    "\n",
    "    if wi.isalnum():\n",
    "        gaz = False\n",
    "        gazplus1 = False\n",
    "        gazminus1 = False\n",
    "        gaz3 = False\n",
    "        for k in g_dict.keys():\n",
    "            if wi in g_dict[k]:\n",
    "                gaz = True\n",
    "                features.update({\n",
    "                    'gaztag-'+str(k): 1,\n",
    "                })\n",
    "            if 'wiplus1' in locals() and wi+' '+wiplus1 in g_dict[k]:\n",
    "                gazplus1 = True\n",
    "                features.update({\n",
    "                    'gaz+1tag-'+str(k): 1,\n",
    "                })\n",
    "            if 'wiminus1' in locals() and wiminus1+' '+wi in g_dict[k]:\n",
    "                gazminus1 = True\n",
    "                features.update({\n",
    "                    'gaz-1tag-'+str(k): 1,\n",
    "                })\n",
    "            if 'wiminus1' in locals()  and 'wiplus1' in locals() and wiminus1+' '+wi+' '+wiplus1 in g_dict[k]:\n",
    "                gaz3 = True\n",
    "                features.update({\n",
    "                    'gaz3tag-'+str(k): 1,\n",
    "                })\n",
    "        features.update({'gaz': gaz , 'gaz+1': gazplus1 , 'gaz-1': gazminus1 , 'gaz3': gaz3})\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sentence):\n",
    "    assert isinstance(sentence, list) and isinstance(sentence[0], str), '`sentence` should be list of words as str'\n",
    "    xs = [None]*len(sentence)\n",
    "    for i in range(len(sentence)):\n",
    "        xs[i] = word2features(i, sentence)\n",
    "    return xs\n",
    "\n",
    "red2v = DictVectorizer(sparse=False)\n",
    "def feats2vects(features, test=False):\n",
    "    if not test:\n",
    "        tX = red2v.fit_transform(features)\n",
    "    elif test:\n",
    "        assert 'red2v' in globals() , 'no fit done earlier for dict to vect'\n",
    "        tX = red2v.transform(features)\n",
    "    return tX\n",
    "\n",
    "def encode_ylabel(y):\n",
    "    ty = [1 if label.startswith('B-') or label.startswith('I-') else 0 for label in y]\n",
    "    return ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "def iob_sents2Xy(iob_sents, test=False) :\n",
    "    sents_list = [['<START>']+[w for w, _, _ in wseq]+['<STOP>'] for wseq in iob_sents]\n",
    "    s2fs = list(map(sent2features, sents_list))\n",
    "    Xs = list(chain.from_iterable(s2fs))\n",
    "    y = list(chain.from_iterable([['<START>']+[e for _, _, e in wseq]+['<STOP>'] for wseq in iob_sents]))\n",
    "    y = encode_ylabel(y)\n",
    "    X = feats2vects(Xs, test=test)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7718, 61), (7718,))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 500\n",
    "Xtrain, ytrain = iob_sents2Xy(train.iob_sents()[:m])\n",
    "Xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 correct out of 195 entities. Accuracy = 0.958974358974359\n",
      "[0 0 0 0 0 0 0 0 0 0] [0 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "svmclassifier = SVC(probability=True)\n",
    "\n",
    "svmclassifier.fit(Xtrain,ytrain)\n",
    "\n",
    "Xtest, ytest = iob_sents2Xy(train.iob_sents()[m:m+10], test=True)\n",
    "\n",
    "predictions = svmclassifier.predict(Xtest)\n",
    "\n",
    "print(f'{(predictions == ytest).sum()} correct out of {ytest.shape[0]} entities. '\n",
    "      f'Accuracy = {(predictions == ytest).sum()/ytest.shape[0]}')\n",
    "print(predictions[:10], ytest[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sent = \"Ludwig van Beethoven: G.O.A.T from \\\"Germany\\\" (1800s AD).\"\n",
    "# user_sent = \"Sam\"\n",
    "# s_l = user_sent.split()\n",
    "pattern = r'(\\s|\"|:|,|:|;|\\'|!|\\?|\\(|\\)|\\.$)'\n",
    "s_l = list(filter(lambda x : ('').__ne__(x) and (' ').__ne__(x), re.split(pattern , user_sent)))\n",
    "if s_l[-1] != '.':\n",
    "    s_l.append('.')\n",
    "s_l = ['<START>'] + s_l + ['<STOP>']\n",
    "s2f = sent2features(s_l)\n",
    "x = feats2vects(s2f, test=True)\n",
    "user_nei = svmclassifier.predict(x)\n",
    "for w , e in zip(s_l, user_nei):\n",
    "    print(f\"{w:20s} - {e if e==1 else ''}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97494322, 0.02505678],\n",
       "       [0.02490268, 0.97509732],\n",
       "       [0.96505666, 0.03494334],\n",
       "       [0.0210292 , 0.9789708 ],\n",
       "       [0.98799203, 0.01200797],\n",
       "       [0.99863349, 0.00136651],\n",
       "       [0.05630069, 0.94369931],\n",
       "       [0.9845131 , 0.0154869 ],\n",
       "       [0.20087021, 0.79912979],\n",
       "       [0.98668858, 0.01331142],\n",
       "       [0.97496493, 0.02503507]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmclassifier.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size : 100\n",
      "Training size : 500\n",
      "Training size : 1000\n",
      "Training size : 5000\n",
      "Training size : 10000\n",
      "Training size : 14987\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "all_train = train.iob_sents()\n",
    "fulltrainsize = len(all_train)\n",
    "trainsizes = [100, 500, 1000, 5000, 10000, fulltrainsize]\n",
    "for trainsize in trainsizes:\n",
    "    print(f'Training size : {trainsize}')\n",
    "    Xtrain, ytrain = iob_sents2Xy(all_train[:trainsize])\n",
    "    with open(f'red2v-{trainsize}.pkl', 'wb') as f:\n",
    "        pickle.dump(red2v, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaarya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
