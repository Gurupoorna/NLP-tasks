{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gurupoorna/NLP-tasks/blob/main/KFold_POS-Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCele34D0YMg",
        "outputId": "31eee99d-166d-427f-bfea-909256bed5cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from numba import njit\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x61W5SDi0uRd"
      },
      "outputs": [],
      "source": [
        "# Load word and tag data from Brown corpus\n",
        "word_and_tag = list(brown.tagged_words())\n",
        "\n",
        "# Extract unique words and POS tags\n",
        "words = list(set([word for word, _ in word_and_tag]))\n",
        "pos_tags = list(set([tag for _, tag in word_and_tag]))\n",
        "\n",
        "# Mapping words and POS tags to indices\n",
        "word_to_index = {word: idx for idx, word in enumerate(words)}\n",
        "tag_to_index = {tag: idx for idx, tag in enumerate(pos_tags)}\n",
        "\n",
        "# Convert word-tag pairs into their corresponding indices before passing to JIT function\n",
        "indexed_word_tag_pairs = [\n",
        "    (word_to_index[word], tag_to_index[tag])\n",
        "    for word, tag in word_and_tag\n",
        "]\n",
        "\n",
        "@njit  # Using Numba to optimize the computation\n",
        "def make_sparse_matrix(indexed_word_tag_pairs, num_words, num_tags):\n",
        "    sparse_matrix = np.zeros((num_words, num_tags))\n",
        "    for word_idx, tag_idx in indexed_word_tag_pairs:\n",
        "        sparse_matrix[word_idx, tag_idx] += 1\n",
        "    return sparse_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtu46AXL2g-G"
      },
      "outputs": [],
      "source": [
        "bnm = True\n",
        "\n",
        "sparse_matrix_path = './sparse_matrix.csv'\n",
        "\n",
        "# If the CSV exists or we need to reconstruct the matrix\n",
        "if not os.path.exists(sparse_matrix_path) or bnm:\n",
        "    sparse_matrix = make_sparse_matrix(indexed_word_tag_pairs, len(words), len(pos_tags))\n",
        "\n",
        "    df = pd.DataFrame(sparse_matrix, columns=pos_tags, dtype=np.int32)\n",
        "    df.insert(0, 'Words', words)\n",
        "    df.to_csv(sparse_matrix_path, index=False)\n",
        "else:\n",
        "    df = pd.read_csv(sparse_matrix_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw0xVyP77nsp"
      },
      "outputs": [],
      "source": [
        "def make_transition_matrix_and_initial_probs(tagged_sentences, tag_to_index):\n",
        "    num_tags = len(tag_to_index)\n",
        "\n",
        "    # Initializing the transition matrix and initial probabilities array\n",
        "    transition_matrix = np.zeros((num_tags, num_tags), dtype=np.float64)\n",
        "    initial_probs = np.zeros(num_tags, dtype=np.float64)\n",
        "\n",
        "    for sentence in tagged_sentences:\n",
        "        # Get the index of the first tag in the sentence for initial probabilities\n",
        "        initial_probs[tag_to_index[sentence[0][1]]] += 1\n",
        "\n",
        "        for n in range(len(sentence) - 1):\n",
        "            tag_i = sentence[n][1]\n",
        "            tag_j = sentence[n + 1][1]\n",
        "\n",
        "            transition_matrix[tag_to_index[tag_i], tag_to_index[tag_j]] += 1\n",
        "\n",
        "    # Normalize the transition matrix row-wise, handling cases where the row sums to 0\n",
        "    row_sums = np.sum(transition_matrix, axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1  # Avoid division by zero for tags with no transitions\n",
        "    transition_matrix /= row_sums\n",
        "\n",
        "    # Normalize initial probabilities, handling zero-sum case\n",
        "    if np.sum(initial_probs) > 0:\n",
        "        initial_probs /= np.sum(initial_probs)\n",
        "\n",
        "    return transition_matrix, initial_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj1Lv6h58fNh"
      },
      "outputs": [],
      "source": [
        "# Generating transition matrix and initial probabilities array\n",
        "A, Pi = make_transition_matrix_and_initial_probs(brown.tagged_sents(categories='news'), tag_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ooG3z8s842B"
      },
      "outputs": [],
      "source": [
        "# Extracting the emission probabilities\n",
        "B = df.iloc[:, 1:].values.astype(np.float64).T\n",
        "B /= np.sum(B, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0nhQPR-4WK0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import jit\n",
        "\n",
        "@jit(nopython=True)\n",
        "def viterbi_algorithm_log(state_transition_matrix, initial_probs, emission_matrix, observations):\n",
        "    \"\"\"A unique Viterbi algorithm for finding the most likely sequence of hidden states using log probabilities.\n",
        "\n",
        "    Args:\n",
        "        state_transition_matrix (np.ndarray): State transition probability matrix (I x I)\n",
        "        initial_probs (np.ndarray): Initial state distribution (I)\n",
        "        emission_matrix (np.ndarray): Emission matrix (I x K)\n",
        "        observations (np.ndarray): Observation sequence (length N)\n",
        "\n",
        "    Returns:\n",
        "        optimal_states (np.ndarray): Optimal state sequence (length N)\n",
        "        log_probability_matrix (np.ndarray): Log probability matrix\n",
        "        backtrack_matrix (np.ndarray): Backtrack matrix for finding the optimal path\n",
        "    \"\"\"\n",
        "    num_states = state_transition_matrix.shape[0]  # I: Number of states\n",
        "    sequence_length = len(observations)            # N: Length of observation sequence\n",
        "\n",
        "    # Tiny value to prevent log(0) errors\n",
        "    tiny_value = np.finfo(np.float64).tiny\n",
        "\n",
        "    # Take logarithms of all probabilities (handling zero probabilities)\n",
        "    log_transition_matrix = np.log(state_transition_matrix + tiny_value)\n",
        "    log_initial_probs = np.log(initial_probs + tiny_value)\n",
        "    log_emission_matrix = np.log(emission_matrix + tiny_value)\n",
        "\n",
        "    # Initialize log probability and backtrack matrices\n",
        "    log_probability_matrix = np.zeros((num_states, sequence_length), dtype=np.float64)\n",
        "    backtrack_matrix = np.zeros((num_states, sequence_length - 1), dtype=np.int32)\n",
        "\n",
        "    # Initialize first column of the log probability matrix\n",
        "    log_probability_matrix[:, 0] = log_initial_probs + log_emission_matrix[:, observations[0]]\n",
        "\n",
        "    # Dynamic programming: fill in the log probability and backtrack matrices\n",
        "    for t in range(1, sequence_length):\n",
        "        for current_state in range(num_states):\n",
        "            log_transition_values = log_transition_matrix[:, current_state] + log_probability_matrix[:, t - 1]\n",
        "            log_probability_matrix[current_state, t] = np.max(log_transition_values) + log_emission_matrix[current_state, observations[t]]\n",
        "            backtrack_matrix[current_state, t - 1] = np.argmax(log_transition_values)\n",
        "\n",
        "    # Backtracking to find the optimal state sequence\n",
        "    optimal_states = np.zeros(sequence_length, dtype=np.int32)\n",
        "    optimal_states[-1] = np.argmax(log_probability_matrix[:, -1])\n",
        "\n",
        "    for t in range(sequence_length - 2, -1, -1):\n",
        "        optimal_states[t] = backtrack_matrix[optimal_states[t + 1], t]\n",
        "\n",
        "    return optimal_states, log_probability_matrix, backtrack_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuytWWC69SCF",
        "outputId": "9ab3f939-1688-4195-90ae-170db6ccc97b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation sequence:   O  =  [42166 26796 11724 48824 43241 32437 29253  7537 40333 33331 37103 26854\n",
            " 11448 22195 54475  2393  5302 32066   536 26372 45251]\n",
            "Optimal state sequence: S  =  [411 320 319 261  42  26 319  91 226 114 407  26 122 136 122  97 331 390\n",
            " 122  56 267]\n",
            "Correct state sequence: S* =  [411, 320, 319, 261, 42, 26, 319, 91, 226, 114, 407, 26, 122, 238, 122, 97, 331, 390, 122, 56, 267]\n",
            "Do they match:  False\n",
            "The sentence:  ['It', 'is', 'impossible', 'to', 'get', 'a', 'fair', 'trial', 'when', 'some', 'of', 'the', 'defendants', 'made', 'statements', 'involving', 'themselves', 'and', 'others', \"''\", '.']\n",
            "                It         is        impossible        to        get        a  \\\n",
            "Correct  (It, PPS)  (is, BEZ)  (impossible, JJ)  (to, TO)  (get, VB)  (a, AT)   \n",
            "Guessed        PPS        BEZ                JJ        TO         VB       AT   \n",
            "\n",
            "               fair        trial         when         some  ...        the  \\\n",
            "Correct  (fair, JJ)  (trial, NN)  (when, WRB)  (some, DTI)  ...  (the, AT)   \n",
            "Guessed          JJ           NN          WRB          DTI  ...         AT   \n",
            "\n",
            "                defendants         made         statements         involving  \\\n",
            "Correct  (defendants, NNS)  (made, VBD)  (statements, NNS)  (involving, VBG)   \n",
            "Guessed                NNS          VBN                NNS               VBG   \n",
            "\n",
            "                 themselves        and         others        ''       .  \n",
            "Correct  (themselves, PPLS)  (and, CC)  (others, NNS)  ('', '')  (., .)  \n",
            "Guessed                PPLS         CC            NNS        ''       .  \n",
            "\n",
            "[2 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "r = 203\n",
        "test_sent = brown.sents(categories='news')[r]\n",
        "\n",
        "O = []\n",
        "correct_tag_seq = []\n",
        "test_tagged = brown.tagged_sents(categories='news')[r]\n",
        "\n",
        "for word, tag in test_tagged:\n",
        "    if word in word_to_index:\n",
        "        O.append(word_to_index[word])\n",
        "        if tag in tag_to_index:\n",
        "            correct_tag_seq.append(tag_to_index[tag])\n",
        "\n",
        "O = np.array(O, dtype=np.int32)\n",
        "\n",
        "# Performing Viterbi decoding using the Viterbi function\n",
        "opt_state_seq, log_prob_trellis, backtrack_matrix = viterbi_algorithm_log(A, Pi, B, O)\n",
        "\n",
        "print('Observation sequence:   O  = ', O)\n",
        "print('Optimal state sequence: S  = ', opt_state_seq)\n",
        "print('Correct state sequence: S* = ', correct_tag_seq)\n",
        "print(\"Do they match: \", correct_tag_seq == list(opt_state_seq))\n",
        "\n",
        "guessed_tags = [pos_tags[i] for i in opt_state_seq]\n",
        "test_tagged_words = [word for word, _ in test_tagged if word in word_to_index]\n",
        "test_result_df = pd.DataFrame(index=test_tagged_words, columns=['Correct', 'Guessed'], data=zip(test_tagged, guessed_tags)).T\n",
        "\n",
        "print('The sentence: ', test_sent)\n",
        "print(test_result_df.iloc[:, (test_result_df.nunique() != 1).values])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHBf-JacKQUl",
        "outputId": "3ecd1c56-1fe4-47cf-e37e-28f0f3f9795b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold accuracy: 0.8946\n",
            "Fold accuracy: 0.8964\n",
            "Fold accuracy: 0.8977\n",
            "Fold accuracy: 0.8959\n",
            "Fold accuracy: 0.8924\n",
            "Final 5-fold cross-validation accuracy: 0.8954\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define number of folds\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "sentences = brown.sents(categories='news')\n",
        "tagged_sentences = brown.tagged_sents(categories='news')\n",
        "\n",
        "word_sequences = []\n",
        "tag_sequences = []\n",
        "\n",
        "for sent, tagged_sent in zip(sentences, tagged_sentences):\n",
        "    word_sequence = [word_to_index[word] for word in sent if word in word_to_index]\n",
        "    tag_sequence = [tag_to_index[tag] for _, tag in tagged_sent if tag in tag_to_index]\n",
        "\n",
        "    if word_sequence and tag_sequence and len(word_sequence) == len(tag_sequence):\n",
        "        word_sequences.append(word_sequence)\n",
        "        tag_sequences.append(tag_sequence)\n",
        "\n",
        "# To calculate transition and emission matrices based on the training data\n",
        "def calculate_transition_emission_matrix(train_word_sequences, train_tag_sequences):\n",
        "    transition_matrix = np.zeros((len(tag_to_index), len(tag_to_index)), dtype=np.float64)\n",
        "    initial_probs = np.zeros(len(tag_to_index), dtype=np.float64)\n",
        "    emission_matrix = np.zeros((len(tag_to_index), len(word_to_index)), dtype=np.float64)\n",
        "\n",
        "    for word_seq, tag_seq in zip(train_word_sequences, train_tag_sequences):\n",
        "        initial_probs[tag_seq[0]] += 1\n",
        "        for i in range(len(tag_seq)):\n",
        "            emission_matrix[tag_seq[i], word_seq[i]] += 1\n",
        "            if i > 0:\n",
        "                transition_matrix[tag_seq[i-1], tag_seq[i]] += 1\n",
        "\n",
        "    # Normalizing the transition and emission matrices\n",
        "    row_sums_transition = np.sum(transition_matrix, axis=1, keepdims=True)\n",
        "    row_sums_transition[row_sums_transition == 0] = 1\n",
        "    transition_matrix /= row_sums_transition\n",
        "\n",
        "    row_sums_emission = np.sum(emission_matrix, axis=1, keepdims=True)\n",
        "    row_sums_emission[row_sums_emission == 0] = 1\n",
        "    emission_matrix /= row_sums_emission\n",
        "\n",
        "    # Normalizing initial probabilities\n",
        "    initial_probs /= np.sum(initial_probs)\n",
        "\n",
        "    return transition_matrix, initial_probs, emission_matrix\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in kf.split(all_word_sequences):\n",
        "    # Split data into training and testing sets\n",
        "    train_sequences = [word_sequences[i] for i in train_index]\n",
        "    train_tag_sequences = [tag_sequences[i] for i in train_index]\n",
        "    test_sequences = [word_sequences[i] for i in test_index]\n",
        "    test_tag_sequences = [tag_sequences[i] for i in test_index]\n",
        "\n",
        "    A, Pi, B = calculate_transition_emission_matrix(train_sequences, train_tag_sequences)\n",
        "\n",
        "    total_tags = 0\n",
        "    correct_tags = 0\n",
        "\n",
        "    for test_words, correct_tags_seq in zip(test_sequences, test_tag_sequences):\n",
        "        test_words = np.array(test_words, dtype=np.int32)\n",
        "        predicted_tags, _, _ = viterbi_algorithm_log(A, Pi, B, test_words)\n",
        "        correct_tags += np.sum(np.array(predicted_tags) == np.array(correct_tags_seq))\n",
        "        total_tags += len(correct_tags_seq)\n",
        "\n",
        "    # Accuracy for each fold\n",
        "    fold_accuracy = correct_tags / total_tags\n",
        "    accuracies.append(fold_accuracy)\n",
        "    print(f\"Fold accuracy: {fold_accuracy:.4f}\")\n",
        "\n",
        "final_accuracy = np.mean(accuracies)\n",
        "print(f\"Final 5-fold cross-validation accuracy: {final_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNjr20exjglzDVMYs8rbXho",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}