{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrzN0CPQ6nONXKpvSpnk9r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gurupoorna/NLP-tasks/blob/main/NER_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import nltk\n",
        "from nltk.corpus.reader import ConllCorpusReader\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "import re\n",
        "from copy import copy, deepcopy\n",
        "\n",
        "train = ConllCorpusReader('/content', 'eng.train', ['words', 'pos', 'ignore', 'chunk'])\n",
        "test = ConllCorpusReader('/content', 'eng.testa', ['words', 'pos', 'ignore', 'chunk'])\n",
        "\n",
        "def load_gazetteer_dict():\n",
        "    with open('/content/gazetteer.txt') as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [i.strip() for i in lines]\n",
        "        g_dict = defaultdict(set)\n",
        "        for line in lines:\n",
        "            tag, word = line.split()[0], (' ').join(line.split()[1:])\n",
        "            g_dict[tag].add(word)\n",
        "    return g_dict\n",
        "\n",
        "g_dict = load_gazetteer_dict()\n",
        "\n",
        "# Function to generate word-level features\n",
        "def word2features(i, wordseq):\n",
        "    wi = wordseq[i]\n",
        "    features = defaultdict(lambda: -1)\n",
        "\n",
        "    if wi == '<START>' or wi == '<STOP>':\n",
        "        features.update({\n",
        "            wi: True\n",
        "        })\n",
        "        return features\n",
        "\n",
        "    features.update({\n",
        "        'Wiaslower': wi.lower(),\n",
        "        'iswialpha': wi.isalpha(),\n",
        "        'iswititle': wi.istitle(),\n",
        "        'iswiupper': wi.isupper(),\n",
        "        'iswilower': wi.islower(),\n",
        "        'iswidigit': wi.isdigit(),\n",
        "        'iswinumeric': wi.isnumeric(),\n",
        "        'Wishape': len(wi),\n",
        "    })\n",
        "\n",
        "    if i > 1:\n",
        "        wiminus1 = wordseq[i - 1]\n",
        "        features.update({\n",
        "            'iswiminus1title': wiminus1.istitle(),\n",
        "            'iswiminus1upper': wiminus1.isupper(),\n",
        "            'iswiminus1lower': wiminus1.islower(),\n",
        "            'Wi-1aslower': wiminus1.lower(),\n",
        "        })\n",
        "    elif i == 1:\n",
        "        features.update({\n",
        "            'BOS': True,\n",
        "        })\n",
        "\n",
        "    if i < len(wordseq) - 2:\n",
        "        wiplus1 = wordseq[i + 1]\n",
        "        features.update({\n",
        "            'iswiplus1title': wiplus1.istitle(),\n",
        "            'iswiplus1upper': wiplus1.isupper(),\n",
        "            'iswiplus1lower': wiplus1.islower(),\n",
        "            'Wi+1aslower': wiplus1.lower(),\n",
        "        })\n",
        "    elif i == len(wordseq) - 2:\n",
        "        features.update({\n",
        "            'EOS': True,\n",
        "        })\n",
        "\n",
        "    if wi != '.':\n",
        "        gaz = False\n",
        "        for k in g_dict.keys():\n",
        "            if wi in g_dict[k]:\n",
        "                gaz = True\n",
        "                features.update({\n",
        "                    'gaztag-' + str(k): 1,\n",
        "                })\n",
        "        features.update({'gaz': gaz})\n",
        "    else:\n",
        "        features.update({'gaz': -1})\n",
        "\n",
        "    return features\n",
        "\n",
        "# Function to convert sentence into features\n",
        "def sent2features(sentence):\n",
        "    assert isinstance(sentence, list) and isinstance(sentence[0], str), 'sentence should be list of words as str'\n",
        "    xs = [None] * len(sentence)\n",
        "    for i in range(len(sentence)):\n",
        "        xs[i] = word2features(i, sentence)\n",
        "    return xs\n",
        "\n",
        "# Label Encoder to return 1 for B- and I- tags, 0 otherwise\n",
        "def ylabel_encode_decode(y, todo='encode'):\n",
        "    if todo == 'encode':\n",
        "        ty = [1 if label.startswith('B-') or label.startswith('I-') else 0 for label in y]\n",
        "    return ty\n",
        "\n",
        "# Function to vectorize the features and labels from sentences in IOB format\n",
        "def iob_sents2Xy(iob_sents, test=False):\n",
        "    Xs = list(chain.from_iterable([['<START>'] + [w for w, _, _ in wseq] + ['<STOP>'] for wseq in iob_sents]))\n",
        "    y = list(chain.from_iterable([['<START>'] + [e for _, _, e in wseq] + ['<STOP>'] for wseq in iob_sents]))\n",
        "\n",
        "    y = ylabel_encode_decode(y)\n",
        "\n",
        "    X = sent2features(Xs)\n",
        "    X = feats2vects(X, test=test)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "d2v = DictVectorizer(sparse=False)\n",
        "\n",
        "# Function to transform features into vectors\n",
        "def feats2vects(features, test=False):\n",
        "    if not test:\n",
        "        return d2v.fit_transform(features)\n",
        "    else:\n",
        "        return d2v.transform(features)\n",
        "\n",
        "m = 1000\n",
        "\n",
        "# Prepare training data\n",
        "Xtrain, ytrain = iob_sents2Xy(train.iob_sents()[:m])\n",
        "\n",
        "svmclassifier = SVC()\n",
        "svmclassifier.fit(Xtrain, ytrain)\n",
        "\n",
        "Xtest, ytest = iob_sents2Xy(train.iob_sents()[m:m + 100], test=True)\n",
        "\n",
        "predictions = svmclassifier.predict(Xtest)\n",
        "\n",
        "print(f'{(predictions == ytest).sum()} correct out of {ytest.shape[0]} entities. '\n",
        "      f'Accuracy = {(predictions == ytest).sum() / ytest.shape[0]}')\n",
        "\n",
        "print(predictions[:10], ytest[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLDANarisLUL",
        "outputId": "06e2f713-943e-4d11-efca-b679aea27879"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1153 correct out of 1185 entities. Accuracy = 0.9729957805907173\n",
            "[0 0 0 1 1 0 1 0 0 0] [0 0 0 1 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nctiYEK-ulju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}