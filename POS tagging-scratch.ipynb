{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gurupoorna/NLP-tasks/blob/main/POS%20tagging-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tvcBcNfB4Mwh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KObsJQY4Mwi"
      },
      "source": [
        "# Hidden Markov Model\n",
        "|||\n",
        "|---|---|\n",
        "| $Q=q_1q_2\\dots q_N$ | a set of N states|\n",
        "| $A=a_{11}\\dots a_{ij}\\dots a_{NN}$ | a transition probability matrix $A$, each $a_{ij}$ representing the probability of moving from state $i$ to state $j$, s.t. $\\sum_{j=1}^N a_{ij} = 1 \\ \\forall i$|\n",
        "| $B=b_i(o_t)$ | a sequence of observation likelihoods, also called emission probabilities, each expressing the probability of an observation $o_t$ (drawn from a vocabulary $V = v_1, v_2, \\dots, v_V$) being generated from a state $q_i$|\n",
        "|$\\pi = \\pi_1, \\pi_2, \\dots, \\pi_N$| an initial probability distribution over states. $\\pi_i$ is the probability that the Markov chain will start in state $i$. Some states $j$ may have $\\pi_j = 0$, meaning that they cannot be initial states. Also, $\\sum_{i=1}^N \\pi_i = 1$|\n",
        "\n",
        "\n",
        "\n",
        "# Calculating prior probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Elw-edPK4Mwi"
      },
      "outputs": [],
      "source": [
        "words = list(set(brown.words())) # unique list of words\n",
        "pos_tags = list(set(pair[1] for pair in brown.tagged_words())) # extract all unique tags from corpus\n",
        "\n",
        "def make_prob_matrices(tagged_sentences,words,pos_tags):\n",
        "    \"\"\" Contructs probability matrices.        \n",
        "    \n",
        "    Args:\n",
        "        tagged_sentences (List[List[Tuple(str,str)],...] List of sentences, each of which is POS tagged as [(<WORD>, <TAG>),...]\n",
        "        words (List[str]): List of all unique words. The index in this list will be later used for learning.\n",
        "        pos_tags (List[str]): List of all unique POS tags occuring in corpus. Likewise, index will be used for learning\n",
        "\n",
        "    Returns:\n",
        "        A:  Transition probability matrix where A[i,j] := P(pos_tags[j] | pos_tag[i]), shape(# tags,# tags)\n",
        "        B:  Probability matrix where B[i,j] := P(words[j] | pos_tags[i]), shape(# words,# tags)\n",
        "        Pi: Initial tag probability where Pi[i] := P(words[i] | ^), shape(# tags)\n",
        "    \"\"\"\n",
        "    A = np.zeros((len(pos_tags),len(pos_tags)),dtype=np.float64)\n",
        "    B = np.zeros((len(pos_tags),len(words)),dtype=np.float64)\n",
        "    Pi = np.zeros(len(pos_tags),dtype=np.float64)\n",
        "    for sentence in tagged_sentences:\n",
        "        Pi[pos_tags.index(sentence[0][1])] += 1\n",
        "        for n in range(len(sentence)-1):\n",
        "            nth, nnth = sentence[n:n+2]\n",
        "            A[pos_tags.index(nth[1]),pos_tags.index(nnth[1])] += 1\n",
        "            B[pos_tags.index(nth[1]), words.index(nth[0])] += 1\n",
        "        B[pos_tags.index(sentence[-1][1]), words.index(sentence[-1][0])] += 1\n",
        "    print(A,B,Pi)\n",
        "    A /= np.sum(A,axis=1).reshape(-1,1)\n",
        "    B /= np.sum(B,axis=1).reshape(-1,1)\n",
        "    Pi /= np.sum(Pi)\n",
        "    return A, B, Pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9oGBGjq4Mwk"
      },
      "source": [
        "# Viterbi parameters\n",
        "|||\n",
        "|---|---|\n",
        " |$v_{t-1}(i)$ | the previous Viterbi path probability from the previous time step|\n",
        " |$a_{ij}$ | the transition probability from previous state $q_i$ to current state $q_j$|\n",
        " |$b_j(o_t)$| the state observation likelihood of the observation symbol $o_t$ given the current state $j$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the probability values if saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.load('A.npy')\n",
        "B = np.load('B.npy')\n",
        "Pi = np.load('Pi.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3><b><u><font color='magenta'> Run next cell only if necessary </font></u></b></h3>\n",
        "\n",
        "Following computation takes a lot of time (16 min). Execute only if necessary (save it then), else load from saved values, as done above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L03qwGUA4Mwk",
        "outputId": "634125fd-107b-43fc-af44-79f6f4bfc6e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 2. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.000e+00 2.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 1.900e+01 6.600e+01 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.600e+01 0.000e+00\n",
            " 1.000e+00 0.000e+00 1.000e+00 0.000e+00 7.270e+02 0.000e+00 2.300e+02\n",
            " 0.000e+00 0.000e+00 4.000e+00 1.000e+00 1.000e+00 0.000e+00 1.000e+00\n",
            " 6.000e+00 0.000e+00 0.000e+00 0.000e+00 2.670e+02 0.000e+00 1.000e+00\n",
            " 0.000e+00 8.297e+03 0.000e+00 1.000e+00 1.000e+00 3.700e+01 0.000e+00\n",
            " 0.000e+00 0.000e+00 9.000e+00 0.000e+00 0.000e+00 0.000e+00 4.168e+03\n",
            " 4.900e+01 6.000e+00 2.410e+02 3.600e+01 0.000e+00 0.000e+00 0.000e+00\n",
            " 8.340e+02 0.000e+00 1.000e+00 2.100e+01 6.400e+01 4.320e+02 3.146e+03\n",
            " 4.000e+00 0.000e+00 2.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
            " 2.815e+03 1.900e+01 0.000e+00 7.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 3.000e+00 0.000e+00 0.000e+00 1.450e+02 0.000e+00 3.800e+01 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 9.000e+00 0.000e+00 2.000e+01 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 3.000e+00 1.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 1.290e+02 0.000e+00 0.000e+00 9.000e+00\n",
            " 0.000e+00 2.700e+01 0.000e+00 0.000e+00 5.200e+01 0.000e+00 0.000e+00\n",
            " 0.000e+00 1.000e+00 3.900e+01 2.150e+02 0.000e+00 0.000e+00 4.640e+02\n",
            " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 3.000e+00 8.000e+00 0.000e+00\n",
            " 0.000e+00 2.500e+01 1.300e+01 1.020e+02 4.000e+00 0.000e+00 1.340e+02\n",
            " 0.000e+00 0.000e+00 0.000e+00 4.000e+01 2.000e+00 1.800e+01 1.600e+01\n",
            " 2.800e+01 5.935e+03 1.000e+00 0.000e+00 1.000e+00 1.063e+03 0.000e+00\n",
            " 0.000e+00 2.100e+01 1.400e+01 0.000e+00 1.200e+01 5.930e+02 0.000e+00\n",
            " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 3.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 3.500e+01 2.400e+01 0.000e+00\n",
            " 0.000e+00 0.000e+00 2.000e+00 7.000e+00 0.000e+00 3.799e+03 0.000e+00\n",
            " 1.000e+00 6.000e+00 8.430e+02 3.748e+03 0.000e+00 0.000e+00 1.170e+02\n",
            " 0.000e+00 0.000e+00 4.000e+00 0.000e+00 0.000e+00 1.000e+00 1.400e+01\n",
            " 1.000e+00 6.100e+01 0.000e+00 3.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 1.000e+00 2.000e+00 2.820e+02 5.500e+01 0.000e+00 0.000e+00\n",
            " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00 8.010e+02 2.100e+01\n",
            " 3.000e+00 0.000e+00 2.840e+02 0.000e+00 0.000e+00 0.000e+00 2.860e+02\n",
            " 0.000e+00 3.800e+01 2.000e+00 0.000e+00 1.000e+00 1.000e+00 1.900e+01\n",
            " 6.100e+01 0.000e+00 2.000e+01 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.377e+03 0.000e+00 2.000e+00 3.000e+00 0.000e+00 0.000e+00 3.900e+01\n",
            " 1.000e+00 0.000e+00 0.000e+00 2.000e+00 0.000e+00 1.950e+02 0.000e+00\n",
            " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 4.700e+01 1.900e+01 0.000e+00\n",
            " 2.000e+00 0.000e+00 1.043e+03 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 1.500e+01 6.700e+01 0.000e+00 4.160e+02 1.500e+01 0.000e+00\n",
            " 4.792e+03 0.000e+00 0.000e+00 2.000e+01 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 1.800e+01 0.000e+00 1.140e+02 3.200e+01 0.000e+00 0.000e+00\n",
            " 0.000e+00 6.200e+01 0.000e+00 6.200e+01 0.000e+00 2.000e+00 3.000e+00\n",
            " 2.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
            " 0.000e+00 9.630e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 1.000e+00 9.800e+01 0.000e+00 1.000e+00 1.200e+01 2.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 1.000e+00 7.000e+00 0.000e+00 0.000e+00 2.000e+00\n",
            " 0.000e+00 0.000e+00 6.400e+01 0.000e+00 0.000e+00 1.000e+00 5.900e+01\n",
            " 0.000e+00 2.700e+01 6.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 2.000e+00 0.000e+00 3.000e+00 0.000e+00\n",
            " 5.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 7.000e+00 0.000e+00\n",
            " 2.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 8.900e+01 3.200e+01\n",
            " 0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 1.540e+02 0.000e+00\n",
            " 0.000e+00 1.000e+00 2.000e+00 5.000e+00 5.400e+01 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 2.150e+02 0.000e+00 0.000e+00 4.300e+01 3.320e+02\n",
            " 2.700e+01 1.810e+02 1.790e+02 0.000e+00 0.000e+00 3.000e+00 2.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 2.550e+02 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 3.000e+00 4.160e+02 3.000e+00 3.000e+00\n",
            " 0.000e+00 1.500e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+01\n",
            " 1.600e+01 8.400e+01 0.000e+00 0.000e+00 2.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 1.900e+01 0.000e+00 0.000e+00 1.100e+01\n",
            " 0.000e+00 0.000e+00 2.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 3.400e+01 0.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
            " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.700e+02\n",
            " 0.000e+00 3.000e+00 1.633e+03 0.000e+00 2.189e+03 0.000e+00 2.800e+01\n",
            " 0.000e+00 4.500e+01 1.000e+00 0.000e+00 0.000e+00 0.000e+00 6.000e+00\n",
            " 3.000e+01 0.000e+00 2.000e+00]\n",
            "Observation sequence:   O  =  [43772, 42693, 52486, 40905, 9881, 46879, 21718, 15634, 9881, 48304, 21718, 4067, 40293, 32586]\n",
            "Optimal state sequence: S  =  [ 43 238 273 219  55 302 369  77  55 302 369 370 219 143]\n",
            "Correct state sequence: S* =  [ 43 238 273 219  55 302 369  77  55 302 369 370 219 143]\n",
            "Do they match :  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True]\n",
            "The sentence :  ['A', 'number', 'of', 'scattered', '``', 'ayes', \"''\", 'and', '``', 'noes', \"''\", 'was', 'heard', '.']\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: [Correct, Guessed]\n"
          ]
        }
      ],
      "source": [
        "if False : # change it to True while testing\n",
        "    A, B, Pi = make_prob_matrices(brown.tagged_sents(), words, pos_tags)\n",
        "    # Saving the computed matrices\n",
        "    np.save('transission matrix', A)\n",
        "    np.save('emission matrix', B)\n",
        "    np.save('initial probabilities', Pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation sequence:   O  =  [27225, 12580, 6089, 1872, 1126, 54893, 52486, 34124, 32219, 47270, 40311, 32219, 1872, 45612, 56033, 52083, 35304, 16334, 52486, 1872, 45674, 6378, 1872, 27100, 30837, 1872, 9881, 4959, 21718, 21057, 52486, 50568, 15634, 1872, 4226, 16334, 30837, 5012, 20139, 33861, 14467, 32586]\n",
            "Optimal state sequence: S  =  [185 185 116  43 136 410 447 335 273 261 238 273  43 335 410 459 273 250\n",
            " 273  43 302 273  43 238  63  43  55 238 369 238 273 238  77  43 125 250\n",
            "  63  43 152 238 238 143]\n",
            "Correct state sequence: S* =  [185 185 116  43 136 410 447 185 273 261 238 273  43 335 410 459 180 250\n",
            " 273  43 302 273  43 238  63  43  55 238 369 238 273 238  77  43 125 250\n",
            "  63  43 152 238 238 143]\n",
            "Do they match :  [ True  True  True  True  True  True  True False  True  True  True  True\n",
            "  True  True  True  True False  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True]\n",
            "The sentence :  ['Mr.', 'Reama', 'told', 'the', 'Rotary', 'Club', 'of', 'Providence', 'at', 'its', 'luncheon', 'at', 'the', 'Sheraton-Biltmore', 'Hotel', 'that', 'about', 'half', 'of', 'the', 'people', 'in', 'the', 'country', 'want', 'the', '``', 'welfare', \"''\", 'type', 'of', 'government', 'and', 'the', 'other', 'half', 'want', 'a', 'free', 'enterprise', 'system', '.']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Providence</th>\n",
              "      <th>about</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Correct</th>\n",
              "      <td>NP</td>\n",
              "      <td>RB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Guessed</th>\n",
              "      <td>NP-TL</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Providence about\n",
              "Correct         NP    RB\n",
              "Guessed      NP-TL    IN"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r = 440              # sample number to test\n",
        "test_sent = brown.sents()[r]\n",
        "O = [words.index(word) for word in test_sent] # producing observation states (words) in terms of its word index\n",
        "test_tagged = [tag for _, tag in brown.tagged_sents()[r]]\n",
        "correct_tag_seq = np.array([pos_tags.index(pair[1]) for pair in brown.tagged_sents()[r]])\n",
        "\n",
        "from viterbi import viterbi_log\n",
        "opt_state_seq, log_prob_trellis, backtrack_matrix = viterbi_log(A,Pi,B,O)\n",
        "\n",
        "# The following was to check if it was working or not. These parts need to be better done.\n",
        "print('Observation sequence:   O  = ', O)\n",
        "print('Optimal state sequence: S  = ', opt_state_seq)\n",
        "print('Correct state sequence: S* = ', correct_tag_seq)\n",
        "print(\"Do they match : \", correct_tag_seq==opt_state_seq)\n",
        "\n",
        "guessed_tags = [pos_tags[i] for i in opt_state_seq]\n",
        "from pandas import DataFrame\n",
        "test_result_df = DataFrame(index=test_sent,columns=['Correct','Guessed'],data=zip(test_tagged,guessed_tags)).T\n",
        "print('The sentence : ', test_sent)\n",
        "test_result_df.iloc[:,(test_result_df.nunique()!=1).values]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
<<<<<<< HEAD
      "version": "3.11.9"
=======
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
>>>>>>> d064a672005c09ae6b0e996f90944648b21747db
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
